[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Definitely the coolest and most useful website in recent years",
    "section": "",
    "text": "Player of Fortnite\n\n\nBA in Computer Science and Mathematics 2024 | St. Olaf College"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Nolan Dowling. I’m from Riverside, Illinois, a suburb of Chicago. In my free time, I like to play listen to music, play video games with friends, and draw."
  },
  {
    "objectID": "index.html#nolan-dowling",
    "href": "index.html#nolan-dowling",
    "title": "Definitely the coolest and most useful website in recent years",
    "section": "",
    "text": "Player of Fortnite\n\n\nBA in Computer Science and Mathematics 2024 | St. Olaf College"
  },
  {
    "objectID": "USStates.html",
    "href": "USStates.html",
    "title": "Mini Project #1: US States",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mdsr)\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nlibrary(lubridate)\nlibrary(statebins)\nlibrary(poliscidata)\nlibrary(viridis)\n\nsample_data &lt;- as_tibble(USArrests) |&gt;\n  mutate(state = tolower(rownames(USArrests)))\n\nus_states_data &lt;- us_states |&gt;\n  left_join(sample_data, by = c(\"region\" = \"state\"))\n\nnolanStates_choropleth &lt;- us_states_data |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = Murder), color = \"black\", size = 0.2) + \n  labs(title = \"U.S. Murder Arrests per 100,000 Residents by State\") +\n  coord_map()\n\nnolanStates_choropleth &lt;- nolanStates_choropleth +\n  scale_fill_viridis(name = \"Murder Rate\", na.value = \"gray\") +\n  theme_minimal()\n\nnolanStates_choropleth"
  },
  {
    "objectID": "MiniProject1/USStates.html",
    "href": "MiniProject1/USStates.html",
    "title": "Mini Project #1: US States",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mdsr)\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nlibrary(lubridate)\nlibrary(statebins)\nlibrary(poliscidata)\nlibrary(viridis)\n\nsample_data &lt;- as_tibble(USArrests) |&gt;\n  mutate(state = tolower(rownames(USArrests)))\n\nus_states_data &lt;- us_states |&gt;\n  left_join(sample_data, by = c(\"region\" = \"state\"))\n\nnolanStates_choropleth &lt;- us_states_data |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = Murder), color = \"black\", size = 0.2) + \n  labs(title = \"U.S. Murder Arrests per 100,000 Residents by State\") +\n  coord_map()\n\nnolanStates_choropleth &lt;- nolanStates_choropleth +\n  scale_fill_viridis(name = \"Murder Rate\", na.value = \"gray\") +\n  theme_minimal()\n\nnolanStates_choropleth"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nMini Project #3: New Year’s Resolutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project #2: Simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project #1: Wisconsin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project #1: US States\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/MiniProject1/index.html",
    "href": "projects/MiniProject1/index.html",
    "title": "Maps",
    "section": "",
    "text": "US States"
  },
  {
    "objectID": "projects/MiniProject1/index.html#maps",
    "href": "projects/MiniProject1/index.html#maps",
    "title": "Maps",
    "section": "",
    "text": "US States"
  },
  {
    "objectID": "docs/MiniProject1/index.html",
    "href": "docs/MiniProject1/index.html",
    "title": "Maps",
    "section": "",
    "text": "US States"
  },
  {
    "objectID": "docs/MiniProject1/index.html#maps",
    "href": "docs/MiniProject1/index.html#maps",
    "title": "Maps",
    "section": "",
    "text": "US States"
  },
  {
    "objectID": "projects/MiniProject1/USStates.html",
    "href": "projects/MiniProject1/USStates.html",
    "title": "Mini Project #1: US States",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mdsr)\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nlibrary(lubridate)\nlibrary(statebins)\nlibrary(poliscidata)\nlibrary(viridis)\n\nsample_data &lt;- as_tibble(USArrests) |&gt;\n  mutate(state = tolower(rownames(USArrests)))\n\nus_states_data &lt;- us_states |&gt;\n  left_join(sample_data, by = c(\"region\" = \"state\"))\n\nnolanStates_choropleth &lt;- us_states_data |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = Murder), color = \"black\", size = 0.2) + \n  labs(title = \"U.S. Murder Arrests per 100,000 Residents by State\") +\n  coord_map()\n\nnolanStates_choropleth &lt;- nolanStates_choropleth +\n  scale_fill_viridis(name = \"Murder Rate\", na.value = \"gray\") +\n  theme_minimal()\n\nnolanStates_choropleth"
  },
  {
    "objectID": "projects/MiniProject1/Wisconsin.html",
    "href": "projects/MiniProject1/Wisconsin.html",
    "title": "Mini Project #1: Wisconsin",
    "section": "",
    "text": "library(sf)\nlibrary(fs)\nlibrary(tidyverse)\nlibrary(lubridate)\ndevtools::install_github(\"baumer-lab/fec12\")\nlibrary(fec12)\nlibrary(tidyverse)\nlibrary(mdsr) \nlibrary(sf)\nlibrary(ggspatial)\nlibrary(prettymapr)\n\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\nst_layers(dsn_districts)\n\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\nhead(districts, width = Inf)\nclass(districts)\n\nwi_shp &lt;- districts |&gt;\n  filter(STATENAME == \"Wisconsin\")\n\ndistrict_elections &lt;- results_house |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"R\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\n\nwi_results &lt;- district_elections |&gt;\n  filter(state == \"WI\") |&gt;\n  select(-state)\n\nwi_merged &lt;- wi_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(wi_results, by = c(\"DISTRICT\" = \"district\"))\nhead(wi_merged, width = Inf)\n\nwi &lt;- ggplot(data = wi_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT), fill = \"white\") + \n  theme_void()\n\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.3.3\n\npal &lt;- colorNumeric(palette = \"RdBu\", domain = c(0, 1))\n\nleaflet_wi &lt;- leaflet(wi_merged) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 1, fillOpacity = 0.7, \n    color = ~pal(1 - r_prop),   \n    popup = ~paste(\"District\", DISTRICT, \"&lt;/br&gt;\", round(r_prop, 4))\n  ) |&gt;                     \n  setView(lng = -90, lat = 44, zoom = 6)\nleaflet_wi"
  },
  {
    "objectID": "projects/MiniProject1/MiniProject2.html",
    "href": "projects/MiniProject1/MiniProject2.html",
    "title": "Mini Project #2: Simulation",
    "section": "",
    "text": "This project simulates 8-player matches of the video game Super Smash Bros. Ultimate, in order to find the probability that players pick the same character as another player.\n\nFirst, we create the num_player variable and set it to 8, to represent the number of players in each game. Then we create the character vector roster, which contains all 86 playable characters in the game.\n\nnum_players &lt;- 8\nroster &lt;- c(\"Mario\", \"Donkey Kong\", \"Link\", \"Samus\", \"Dark Samus\", \"Yoshi\", \"Kirby\", \"Fox\", \"Pikachu\", \"Luigi\", \"Ness\", \"Captain Falcon\", \"Jigglypuff\", \"Peach\", \"Daisy\", \"Bowser\", \"Ice Climbers\", \"Sheik\", \"Zelda\", \"Dr. Mario\", \"Pichu\", \"Falco\", \"Marth\", \"Lucina\", \"Young Link\", \"Ganondorf\", \"Mewtwo\", \"Roy\", \"Chrom\", \"Mr. Game & Watch\", \"Meta Knight\", \"Pit\", \"Dark Pit\", \"Zero Suit Samus\", \"Wario\", \"Snake\", \"Ike\", \"Pokémon Trainer\", \"Diddy Kong\", \"Lucas\", \"Sonic\", \"King Dedede\", \"Olimar\", \"Lucario\", \"R.O.B.\", \"Toon Link\", \"Wolf\", \"Villager\", \"Mega Man\", \"Wii Fit Trainer\", \"Rosalina & Luma\", \"Little Mac\", \"Greninja\", \"Mii Brawler\", \"Mii Swordfighter\", \"Mii Gunner\", \"Palutena\", \"Pac-Man\", \"Robin\", \"Shulk\", \"Bowser Jr.\", \"Duck Hunt\", \"Ryu\", \"Ken\", \"Cloud\", \"Corrin\", \"Bayonetta\", \"Inkling\", \"Ridley\", \"Simon\", \"Richter\", \"King K. Rool\", \"Isabelle\", \"Incineroar\", \"Piranha Plant\", \"Joker\", \"Hero\", \"Banjo & Kazooie\", \"Terry\", \"Byleth\", \"Min Min\", \"Steve\", \"Sephiroth\", \"Pyra/Mythra\", \"Kazuya\", \"Sora\")\n\nThen we define the function simulate_game, which simulates one match of the game, where the players choose from the roster.\n\nsimulate_game &lt;- function(num_players, roster) {\n  player_selections &lt;- sample(roster, num_players, replace = TRUE)\n  return(sum(duplicated(player_selections)))\n}\n\nNow, we run this simulation num_simulations times . Throughout the simulations, we count how many games had at least one match. Then we print those values. Additionally, we create a numeric vector match_counts of length num_simulations, to store how many matches occur each simulation.\n\nnum_simulations &lt;- 100000\ncollision_count &lt;- 0\nmatch_counts &lt;- numeric(num_simulations)\n\nfor (i in 1:num_simulations) {\n  if (simulate_game(num_players, roster) &gt; 0) {\n    collision_count &lt;- collision_count + 1\n  }\n  match_counts[i] &lt;- simulate_game(num_players, roster)\n}\n\ncollision_probability &lt;- collision_count / num_simulations\n\ncat(\"Number of simulations with collisions:\", collision_count, \"\\n\")\n\nNumber of simulations with collisions: 28231 \n\ncat(\"Probability of selecting the same character:\", collision_probability, \"\\n\")\n\nProbability of selecting the same character: 0.28231 \n\n\nNext we create a tibble using the data from those simulations…\n\ncollision_data &lt;- tibble(\n  Match_Outcome = c(\"Match\", \"No Match\"),\n  Frequency = c(collision_count, num_simulations - collision_count)\n)\ncollision_data\n\n# A tibble: 2 × 2\n  Match_Outcome Frequency\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Match             28231\n2 No Match          71769\n\n\n…and use that tibble to create a plot to show the proportion of games where matches occurred.\n\nggplot(collision_data, aes(x = Match_Outcome, y = Frequency, fill = Match_Outcome)) +\n  geom_bar(stat = \"identity\", fill = \"red\", color = \"white\", width =0.7) +\n  labs(\n    title = \"Frequency of Selecting the Same Character in an 8-Player Game\",\n    x = \"Match Outcome\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray'))\n\n\n\n\nHere we create a tibble to show the distribution of collision counts per simulation.\n\ncollision_counts &lt;- replicate(num_simulations, simulate_game(num_players, roster))\ncollision_df &lt;- tibble(Match_Count = collision_counts)\n\ncollision_df |&gt;\n  count(Match_Count)\n\n# A tibble: 5 × 2\n  Match_Count     n\n        &lt;int&gt; &lt;int&gt;\n1           0 71546\n2           1 25259\n3           2  3027\n4           3   164\n5           4     4\n\n\nFinally, we plot this data in a histogram.\n\ncollision_df &lt;- collision_df |&gt;\n  count(Match_Count) |&gt;\n  mutate(Proportion = n / sum(n))\n\nggplot(collision_df, aes(x = Match_Count, y = Proportion)) +\n  geom_bar(stat = \"identity\", fill = \"red\", color = \"white\", width = 0.5) +\n  labs(\n    title = \"Distribution of Matches per Simulation\",\n    x = \"Matches Per Game\",\n    y = \"Proportion of Games\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray'))\n\n\n\n\nFrom all of these simulations and graphs, we have visualized the probability that any players pick the same character in an 8-player game of Super Smash Bros. Ultimate. Also, we have seen the proportions of match counts throughout all of the simulated games. Although this data is interesting to analyze, the probabilities calculated may not be fully reflective of reality, as actual players may have preferences for certain characters, which means the probabilities could be drastically different. However, in a fully random scenario, such as when all players select character fully at random, these plots would display those same probabilities."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Nolan Dowling",
    "section": "",
    "text": "hi"
  },
  {
    "objectID": "projects/MiniProject1/MiniProject3.html",
    "href": "projects/MiniProject1/MiniProject3.html",
    "title": "Mini Project #3: New Year’s Resolutions",
    "section": "",
    "text": "This project uses regular expressions to perform sentiment analysis on a sample of 5,002 New Year’s Resolution tweets from the following source.\n\nNew_years_resolutions_DFE &lt;- read.csv(\"C:/Users/Nolan/OneDrive/Documents/Data Science 2/New-years-resolutions-DFE.csv\")\n\nTo prepare the data, we start by removing the sentiment analysis already completed. We want to ignore the topics that the creator of the dataset has made, so I will drop those columns, and instead only care about the actual Tweets and the data surrounding the creators of the Tweets.\n\nnew_years &lt;- New_years_resolutions_DFE |&gt;\n  select(-other_topic, -resolution_topics, -Resolution_Category)\nnew_years$text &lt;- iconv(new_years$text, \"UTF-8\", \"ASCII\", sub = \" \")\n\nA common occurrence in New Year’s Resolutions is for somebody to end one of their bad habits. To do this, we take the cleaned data and sort it for words “don’t”, “not”, and “stop”. Then we put the counts of these words and graph the proportions of tweets with these words.\n\nall_tweets &lt;- paste(new_years$text, collapse = \" \")\n\nmatches &lt;- str_extract_all(all_tweets, \"\\\\b(avoid|quit|give up|not|don't|stop|less)\\\\b\")\n\nall_matches &lt;- unlist(matches)\n\nword_counts &lt;- table(all_matches)\n\nword_counts_tbl &lt;- as.tibble(word_counts)\ncolnames(word_counts_tbl) &lt;- c(\"Word\", \"Frequency\")\n\nword_counts_tbl$Word &lt;- fct_reorder(word_counts_tbl$Word, word_counts_tbl$Frequency)\n\nggplot(word_counts_tbl, aes(x = Word, y = Frequency)) +\n  geom_bar(stat = \"identity\", fill = \"red\", color = \"white\") +\n  labs(title = \"Frequency of negative terms\",\n       x = \"Word\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\",\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray'))\n\n\n\n\nWe can see here that a large proportion of the Tweets made included these negative words. What if we search for the opposite, where people are trying to start up a healthy habit, or try something new? Let’s search for the words “start”, “try”, and “begin”.\n\nmatches &lt;- str_extract_all(all_tweets, \"\\\\b(grow|improve|develop|expand|start|try|begin|learn|keep)\\\\b\")\n\nall_matches &lt;- unlist(matches)\n\nword_counts &lt;- table(all_matches)\n\nword_counts_tbl &lt;- as.tibble(word_counts)\ncolnames(word_counts_tbl) &lt;- c(\"Word\", \"Frequency\")\n\nword_counts_tbl$Word &lt;- fct_reorder(word_counts_tbl$Word, word_counts_tbl$Frequency)\n\nggplot(word_counts_tbl, aes(x = Word, y = Frequency)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"white\") +\n  labs(title = \"Frequency of positive terms\",\n       x = \"Word\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\",\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray'))\n\n\n\n\nThe dataset also includes which region the Tweet was made from. We can organize the total positive and negative terms, and display the counts of these terms among the regions.\n\nregion_counts &lt;- new_years |&gt;\n  mutate(\n    region = str_to_title(tolower(tweet_region))\n  ) |&gt;\n  group_by(region) |&gt;\n  summarise(\n    negative_terms = sum(str_count(tolower(text), \"\\\\b(avoid|quit|give up|not|don't|stop)\\\\b\")),\n    positive_terms = sum(str_count(tolower(text), \"\\\\b(grow|improve|develop|expand|start|try|begin|learn|keep)\\\\b\"))\n  )\n\nregion_counts_long &lt;- pivot_longer(region_counts, cols = c(negative_terms, positive_terms), names_to = \"Term Type\", values_to = \"Frequency\")\n\nggplot(region_counts_long, aes(x = region, y = Frequency, fill = `Term Type`)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n  labs(title = \"Frequency of Positive/Negative Terms by Region\",\n       x = \"Region\",\n       y = \"Frequency\",\n       fill = \"Term Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray')) +\n  scale_fill_manual(values = c(\"negative_terms\" = \"red\", \"positive_terms\" = \"blue\"),\n                    labels = c(\"Negative terms\", \"Positive terms\"))\n\n\n\n\nWe can also delve deeper into more specific terms, instead of just positive or negative. Here, we search for a variety of the most common resolution topics, such as “gym”, “weight”, “drinking”, and “diet”.\n\nmatches &lt;- str_extract_all(all_tweets, \"\\\\b(gym|weight|drinking|alcohol|smoking|lazy|health|fitness|diet|active|job|guitar|travel|love|watch)\\\\b\")\n\nall_matches &lt;- unlist(matches)\n\nword_counts &lt;- table(all_matches)\n\nword_counts_tbl &lt;- as.tibble(word_counts)\ncolnames(word_counts_tbl) &lt;- c(\"Word\", \"Frequency\")\n\nword_counts_tbl$Word &lt;- fct_reorder(word_counts_tbl$Word, word_counts_tbl$Frequency)\n\nggplot(word_counts_tbl, aes(x = Word, y = Frequency)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"white\") +\n  labs(title = \"Frequency of specific resolution terms\",\n       x = \"Word\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\",\n        panel.background = element_rect(fill = 'black', color = 'black'),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        text = element_text(color = 'lightgray'),\n        axis.text = element_text(color = 'lightgray'))\n\n\n\n\nIn conclusion, we can see that more New Year’s Resolutions consist of ending bad habits compared to starting new ones. Also, among the words pertaining to specific resolution topics, the most common words are “love”, “gym”, “smoking”, and “weight”."
  }
]